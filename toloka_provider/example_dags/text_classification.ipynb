{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text classification process using Toloka running at Airflow\n",
    "\n",
    "Toloka offers a library of Airflow-integrated functions to facilitate crowdsourcing.\n",
    "This example illustrates how one can build the whole project using these blocks.\n",
    "\n",
    "This library provides Airflow tasks for Toloka.\n",
    "You can connect tasks by passing one task's result to another as argument. For more details see Airflow docs.\n",
    "\n",
    "Airflow can be run either locally or in docker with possibility for distribution.\n",
    "We recommend to use docker, and it will be used in our example.\n",
    "\n",
    "To get acquainted with Toloka tools for free,\n",
    "you can use the promo code **TOLOKAKIT1** on $20 on your [profile page](https://toloka.yandex.com/requester/profile?utm_source=github&utm_medium=site&utm_campaign=tolokakit) after registration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Airflow configuration\n",
    "\n",
    "First, you need to do basic configuration for running Airflow in docker.\n",
    "\n",
    "Follow the instruction below. If you have any troubles, see [docs](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html).\n",
    "\n",
    "1. Install [Docker Community Edition (CE)](https://docs.docker.com/engine/install/) on your workstation.\n",
    "Depending on the OS, you may need to configure your Docker instance to use 4.00 GB of memory for all containers to run properly.\n",
    "Please refer to the Resources section if using [Docker for Windows](https://docs.docker.com/desktop/windows/) or [Docker for Mac](https://docs.docker.com/desktop/mac/) for more information.\n",
    "2. Install [Docker Compose v1.29.1](https://docs.docker.com/compose/install/) and newer on your workstation.\n",
    "    > Default amount of memory available for Docker on MacOS is often not enough to get Airflow up and running.\n",
    "    **If enough memory is not allocated, it might lead to airflow webserver continuously restarting.\n",
    "    You should at least allocate 4GB memory for the Docker Engine (ideally 8GB). You can check and change the amount of memory in [Resources](https://docs.docker.com/desktop/mac/)**\n",
    "3. Create airflow folder\n",
    "```\n",
    "mkdir airflow && cd airflow\n",
    "```\n",
    "4. Fetch [docker-compose.yaml](https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml)\n",
    "```\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.4/docker-compose.yaml'\n",
    "```\n",
    "5. Create custom Dockerfile with commands for installation required packages:\n",
    "```\n",
    "FROM apache/airflow:2.2.4-python3.9\n",
    "RUN pip install --no-cache-dir airflow-provider-toloka\n",
    "RUN pip install --no-cache-dir toloka-kit\n",
    "RUN pip install --no-cache-dir crowd-kit\n",
    "RUN pip install --no-cache-dir pandas\n",
    "```\n",
    "or can find Dockerfile in our files.\n",
    "To build custom docker-image invoke the command below:\n",
    "```\n",
    "docker build . -f Dockerfile --tag toloka-airflow-image\n",
    "```\n",
    "6. Set docker image name in console\n",
    "```\n",
    "echo \"AIRFLOW_IMAGE_NAME=toloka-airflow-image:latest\" >> .env\n",
    "```\n",
    "or replace it in docker-compose file\n",
    "```\n",
    "✕ image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.4}\n",
    "✓ image: ${AIRFLOW_IMAGE_NAME:-toloka-airflow-image:latest}\n",
    "```\n",
    "7. On Linux, the quick-start needs to know your host user id and needs to have group id set to 0.\n",
    "Otherwise the files created in dags, logs and plugins will be created with root user. You have to make sure to configure them for the docker-compose:\n",
    "```\n",
    "mkdir -p ./dags ./logs ./plugins\n",
    "echo -e \"AIRFLOW_UID=$(id -u)\" >> .env\n",
    "```\n",
    "For other operating systems, you will get warning that AIRFLOW_UID is not set, but you can ignore it. You can also manually create the .env file in the same folder your docker-compose.yaml is placed with this content to get rid of the warning:\n",
    "```\n",
    "AIRFLOW_UID=50000\n",
    "```\n",
    "\n",
    "8. Place file \"text_classification.py\" in `dags/` folder to let Airflow see it.\n",
    "\n",
    "9. On all operating systems, you need to run database migrations and create the first user account. To do it, run.\n",
    "```\n",
    "docker-compose up airflow-init\n",
    "```\n",
    "\n",
    "10. Now you can start all services:\n",
    "```\n",
    "docker-compose up\n",
    "```\n",
    "The Airflow will be available at http://localhost:8080/home.\n",
    "Default login and password are \"airflow\" and \"airflow\".\n",
    "\n",
    "### Store Toloka Credentials\n",
    "\n",
    "To use Toloka from Airflow you should pass a Toloka OAuth token to it.\n",
    "If you do not have it yet, you can obtain it from your Toloka account at the `Profile / External Services Integration` page by clicking at `Get OAuth token`.\n",
    "\n",
    "![Account information](./images/some_account.png)\n",
    "\n",
    "You can store it in Airflow as a connection.\n",
    "\n",
    "![Variable location](./images/connections.png)\n",
    "\n",
    "Add a new connection at `Admin / Connections` page. Name it `toloka_default` and write Toloka token in password field.\n",
    "Use fernet for proper security, see [docs](https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html).\n",
    "\n",
    "![Token variable](./images/toloka_connection.png)\n",
    "\n",
    "You can choose any connection type if `toloka` doesn't exist."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text classification project\n",
    "\n",
    "Configuration is finished. Here you can see the whole \"text_classification.py\".\n",
    "\n",
    "```python\n",
    "from datetime import timedelta\n",
    "import json\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "import toloka_provider.tasks.toloka as tlk_tasks\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': days_ago(5),\n",
    "    'retries': 0,\n",
    "}\n",
    "\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, catchup=False, tags=['example'])\n",
    "def text_classification():\n",
    "\n",
    "    @task\n",
    "    def download_json(url):\n",
    "        \"\"\"Download and parse json config stored at given url.\"\"\"\n",
    "        import requests\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    @task(multiple_outputs=True)\n",
    "    def prepare_datasets(unlabeled_url: str, labeled_url: str):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import pandas as pd\n",
    "\n",
    "        labeled = pd.read_csv(labeled_url)\n",
    "        labeled, exam_tasks = train_test_split(labeled, test_size=10, stratify=labeled.category)\n",
    "        _, honeypots = train_test_split(labeled, test_size=20, stratify=labeled.category)\n",
    "\n",
    "        main_tasks = pd.read_csv(unlabeled_url).sample(n=100)\n",
    "\n",
    "        return {\n",
    "            'main_tasks': main_tasks.to_json(),\n",
    "            'exam_tasks': exam_tasks.to_json(),\n",
    "            'honeypots': honeypots.to_json()\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def prepare_tasks(main_tasks):\n",
    "        main_tasks = json.loads(main_tasks)\n",
    "        return [{'input_values': {'headline': headline}}\n",
    "                for headline in main_tasks['headline'].values()]\n",
    "\n",
    "    @task\n",
    "    def prepare_exam_tasks(exam_tasks):\n",
    "        exam_tasks = json.loads(exam_tasks)\n",
    "        return [{'input_values': {'headline': headline},\n",
    "                 'known_solutions': [{'output_values': {'category': category}}],\n",
    "                 'message_on_unknown_solution': category}\n",
    "                for headline, category in zip(exam_tasks['headline'].values(), exam_tasks['category'].values())]\n",
    "\n",
    "    @task\n",
    "    def prepare_honeypots(honeypots):\n",
    "        honeypots = json.loads(honeypots)\n",
    "        return [{'input_values': {'headline': headline},\n",
    "                 'known_solutions': [{'output_values': {'category': category}}]}\n",
    "                for headline, category in zip(honeypots['headline'].values(), honeypots['category'].values())]\n",
    "\n",
    "    @task\n",
    "    def aggregate_assignments(assignments):\n",
    "        from crowdkit.aggregation import DawidSkene\n",
    "        from toloka.client import structure, Assignment\n",
    "        import pandas as pd\n",
    "\n",
    "        assignments = [Assignment.from_json(assignment) for assignment in assignments]\n",
    "        tasks = []\n",
    "        labels = []\n",
    "        performers = []\n",
    "        for assignment in assignments:\n",
    "            for task, solution in zip(assignment.tasks, assignment.solutions):\n",
    "                tasks.append(task.input_values['headline'])\n",
    "                labels.append(solution.output_values['category'])\n",
    "                performers.append(assignment.user_id)\n",
    "        assignments = {\n",
    "            'task': tasks,\n",
    "            'performer': performers,\n",
    "            'label': labels\n",
    "        }\n",
    "        assignments = pd.DataFrame.from_dict(assignments)\n",
    "\n",
    "        df = DawidSkene(n_iter=20).fit_predict(assignments).to_frame().reset_index()\n",
    "        df.columns = ['headline', 'category']\n",
    "\n",
    "        print('RESULT', df)\n",
    "\n",
    "    project_conf = download_json(\n",
    "        'https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/configs/project.json')\n",
    "    exam_conf = download_json(\n",
    "        'https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/configs/exam.json')\n",
    "    pool_conf = download_json(\n",
    "        'https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/configs/pool.json')\n",
    "\n",
    "    project = tlk_tasks.create_project(project_conf)\n",
    "    exam = tlk_tasks.create_exam_pool(exam_conf, project=project)\n",
    "    pool = tlk_tasks.create_pool(pool_conf, project=project, exam_pool=exam, expiration=timedelta(days=1))\n",
    "\n",
    "    dataset = prepare_datasets(\n",
    "        unlabeled_url='https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/data/not_known.csv',\n",
    "        labeled_url='https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/data/known.csv',\n",
    "    )\n",
    "    main_tasks, exam_tasks, honeypots = dataset['main_tasks'], dataset['exam_tasks'], dataset['honeypots']\n",
    "    tasks = prepare_tasks(main_tasks)\n",
    "    exam_tasks = prepare_exam_tasks(exam_tasks)\n",
    "    honeypots = prepare_honeypots(honeypots)\n",
    "\n",
    "    _exam_upload = tlk_tasks.create_tasks(exam_tasks, pool=exam, additional_args={'open_pool': True, 'allow_defaults': True})\n",
    "    _honeypots_upload = tlk_tasks.create_tasks(honeypots, pool=pool, additional_args={'allow_defaults': True})\n",
    "    _tasks_upload = tlk_tasks.create_tasks(tasks, pool=pool, additional_args={'allow_defaults': True})\n",
    "\n",
    "    opened_pool = tlk_tasks.open_pool(pool)\n",
    "    _waiting = tlk_tasks.wait_pool(opened_pool)\n",
    "\n",
    "    assignments = tlk_tasks.get_assignments(pool)\n",
    "    aggregate_assignments(assignments)\n",
    "\n",
    "    [_exam_upload, _honeypots_upload, _tasks_upload] >> opened_pool\n",
    "    _waiting >> assignments\n",
    "\n",
    "\n",
    "dag = text_classification()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then if you did everything all right and ran docker properly\n",
    "you can go to http://localhost:8080/home and see `text_classification` dag that can be run.\n",
    "\n",
    "![run button location](./images/run_button.png)\n",
    "\n",
    "Then you can see statuses of tasks in graph tab.\n",
    "\n",
    "![dag representation](./images/dag_representation.png)\n",
    "\n",
    "When \"aggregate_assignments\" will be green it means that pipeline is finished.\n",
    "You can click on `aggregate_assignments` then clink on `Log` and you will see some results of aggregation at the bottom of logs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step by step explanation.\n",
    "\n",
    "Basic configuration. `dag` decorator defines airflow dag.\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': days_ago(5),\n",
    "    'retries': 0,\n",
    "}\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, catchup=False, tags=['example'])\n",
    "def text_classification():\n",
    "    ...\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code downloads json configuration for project and pools.\n",
    "We use it to not create project and pools by our own.\n",
    "\n",
    "```python\n",
    "@task\n",
    "def download_json(url):\n",
    "    \"\"\"Download and parse json config stored at given url.\"\"\"\n",
    "    import requests\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are some user-defined functions to prepare input data.\n",
    "\n",
    "```python\n",
    "@task(multiple_outputs=True)\n",
    "def prepare_datasets(unlabeled_url: str, labeled_url: str):\n",
    "    ...\n",
    "\n",
    "@task\n",
    "def prepare_tasks(main_tasks):\n",
    "    ...\n",
    "\n",
    "@task\n",
    "def prepare_exam_tasks(exam_tasks):\n",
    "    ...\n",
    "\n",
    "@task\n",
    "def prepare_honeypots(honeypots):\n",
    "    ...\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`aggregate_assignments` function aggregates results by DawidSkene method.\n",
    "Results will be printed in airflow logs.\n",
    "\n",
    "To aggregate the results, we recommend using the methods of the [crowd-kit](https://github.com/Toloka/crowd-kit) package.\n",
    "```python\n",
    "@task\n",
    "def aggregate_assignments(assignments):\n",
    "    from crowdkit.aggregation import DawidSkene\n",
    "    from toloka.client import structure, Assignment\n",
    "    import pandas as pd\n",
    "\n",
    "    assignments = [Assignment.from_json(assignment) for assignment in assignments]\n",
    "    tasks = []\n",
    "    labels = []\n",
    "    performers = []\n",
    "    for assignment in assignments:\n",
    "        for task, solution in zip(assignment.tasks, assignment.solutions):\n",
    "            tasks.append(task.input_values['headline'])\n",
    "            labels.append(solution.output_values['category'])\n",
    "            performers.append(assignment.user_id)\n",
    "    assignments = {\n",
    "        'task': tasks,\n",
    "        'performer': performers,\n",
    "        'label': labels\n",
    "    }\n",
    "    assignments = pd.DataFrame.from_dict(assignments)\n",
    "\n",
    "    df = DawidSkene(n_iter=20).fit_predict(assignments).to_frame().reset_index()\n",
    "    df.columns = ['headline', 'category']\n",
    "\n",
    "    print('RESULT', df)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define the topology of our pipeline.\n",
    "\n",
    "```python\n",
    "project_conf = download_json(\n",
    "    'https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/configs/project.json')\n",
    "exam_conf = download_json(\n",
    "    'https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/configs/exam.json')\n",
    "pool_conf = download_json(\n",
    "    'https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/configs/pool.json')\n",
    "\n",
    "project = tlk_tasks.create_project(project_conf)\n",
    "exam = tlk_tasks.create_exam_pool(exam_conf, project=project)\n",
    "pool = tlk_tasks.create_pool(pool_conf, project=project, exam_pool=exam, expiration=timedelta(days=1))\n",
    "\n",
    "dataset = prepare_datasets(\n",
    "    unlabeled_url='https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/data/not_known.csv',\n",
    "    labeled_url='https://raw.githubusercontent.com/Toloka/toloka-airflow/main/example/data/known.csv',\n",
    ")\n",
    "main_tasks, exam_tasks, honeypots = dataset['main_tasks'], dataset['exam_tasks'], dataset['honeypots']\n",
    "tasks = prepare_tasks(main_tasks)\n",
    "exam_tasks = prepare_exam_tasks(exam_tasks)\n",
    "honeypots = prepare_honeypots(honeypots)\n",
    "\n",
    "_exam_upload = tlk_tasks.create_tasks(exam_tasks, pool=exam, additional_args={'open_pool': True, 'allow_defaults': True})\n",
    "_honeypots_upload = tlk_tasks.create_tasks(honeypots, pool=pool, additional_args={'allow_defaults': True})\n",
    "_tasks_upload = tlk_tasks.create_tasks(tasks, pool=pool, additional_args={'allow_defaults': True})\n",
    "\n",
    "opened_pool = tlk_tasks.open_pool(pool)\n",
    "_waiting = tlk_tasks.wait_pool(opened_pool)\n",
    "\n",
    "assignments = tlk_tasks.get_assignments(pool)\n",
    "aggregate_assignments(assignments)\n",
    "\n",
    "[_exam_upload, _honeypots_upload, _tasks_upload] >> opened_pool\n",
    "_waiting >> assignments\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What's next?\n",
    "\n",
    "It was just an example of creating a workflow on Airflow with Toloka.\n",
    "Further it can be expanded in the following directions:\n",
    "\n",
    "* For production version you can configure custom XCom backend that lets pass heavy data across tasks. See example in `custom_xcom_backend.py`.\n",
    "* You can [schedule your flow runs with Timetables](https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html).\n",
    "* You can configure [Logging and Monitoring](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-architecture.html).\n",
    "* Apache Airflow aims to be a very Kubernetes-friendly project, you can run Airflow from within\n",
    "a Kubernetes cluster in order to take advantage of the increased stability and autoscaling options that Kubernetes provides, see [docs](https://airflow.apache.org/docs/apache-airflow/stable/kubernetes.html).\n",
    "* And, of course, you can build much more advanced process with crowdsourcing using Toloka."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
